"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3012],{50510:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var t=s(74848),r=s(28453);const i={title:"SCS k8s-cluster-api-provider upgrade guide",version:new Date("2023-09-07T00:00:00.000Z"),authors:"Kurt Garloff, Roman Hros, Matej Feder",state:"Draft (v0.7)"},l=void 0,o={id:"container/components/k8s-cluster-api-provider/doc/Upgrade-Guide",title:"SCS k8s-cluster-api-provider upgrade guide",description:"SCS k8s-cluster-api-provider upgrade guide",source:"@site/docs/03-container/components/k8s-cluster-api-provider/doc/Upgrade-Guide.md",sourceDirName:"03-container/components/k8s-cluster-api-provider/doc",slug:"/container/components/k8s-cluster-api-provider/doc/Upgrade-Guide",permalink:"/docs/container/components/k8s-cluster-api-provider/doc/Upgrade-Guide",draft:!1,unlisted:!1,editUrl:"https://github.com/SovereignCloudStack/docs/tree/main/docs/03-container/components/k8s-cluster-api-provider/doc/Upgrade-Guide.md",tags:[],version:"current",frontMatter:{title:"SCS k8s-cluster-api-provider upgrade guide",version:"2023-09-07T00:00:00.000Z",authors:"Kurt Garloff, Roman Hros, Matej Feder",state:"Draft (v0.7)"},sidebar:"docs",previous:{title:"Ingress with externalTrafficPolicy: local",permalink:"/docs/container/components/k8s-cluster-api-provider/doc/LoadBalancer-ExtTrafficLocal"},next:{title:"Continuous integration",permalink:"/docs/container/components/k8s-cluster-api-provider/doc/continuous-integration"}},a={},c=[{value:"SCS k8s-cluster-api-provider upgrade guide",id:"scs-k8s-cluster-api-provider-upgrade-guide",level:2},{value:"Management host (cluster) vs. Workload clusters",id:"management-host-cluster-vs-workload-clusters",level:2},{value:"Updating the management host",id:"updating-the-management-host",level:2},{value:"In-place upgrade",id:"in-place-upgrade",level:3},{value:"Operating system",id:"operating-system",level:4},{value:"k8s-cluster-api-provider git",id:"k8s-cluster-api-provider-git",level:4},{value:"Updating cluster-API and openstack cluster-API provider",id:"updating-cluster-api-and-openstack-cluster-api-provider",level:4},{value:"New templates",id:"new-templates",level:4},{value:"R2 to R3",id:"r2-to-r3",level:5},{value:"R3 to R4",id:"r3-to-r4",level:5},{value:"R4 to R5",id:"r4-to-r5",level:5},{value:"R5 to R6",id:"r5-to-r6",level:5},{value:"New defaults",id:"new-defaults",level:4},{value:"The clusterctl move approach",id:"the-clusterctl-move-approach",level:3},{value:"Updating workload clusters",id:"updating-workload-clusters",level:2},{value:"k8s version upgrade",id:"k8s-version-upgrade",level:3},{value:"On R2 clusters",id:"on-r2-clusters",level:4},{value:"On R3 and R4 clusters",id:"on-r3-and-r4-clusters",level:4},{value:"On R5 clusters",id:"on-r5-clusters",level:4},{value:"On R6 clusters",id:"on-r6-clusters",level:4},{value:"New versions for mandatory components",id:"new-versions-for-mandatory-components",level:3},{value:"New versions for optional components",id:"new-versions-for-optional-components",level:3},{value:"etcd leader changes",id:"etcd-leader-changes",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"scs-k8s-cluster-api-provider-upgrade-guide",children:"SCS k8s-cluster-api-provider upgrade guide"}),"\n",(0,t.jsx)(n.p,{children:"This document explains the steps to upgrade the SCS Kubernetes cluster-API\nbased cluster management solution as follows:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"from the R2 (2022-03) to the R3 (2022-09) state"}),"\n",(0,t.jsx)(n.li,{children:"from the R3 (2022-09) to the R4 (2023-03) state"}),"\n",(0,t.jsx)(n.li,{children:"from the R4 (2023-03) to the R5 (2023-09) state"}),"\n",(0,t.jsx)(n.li,{children:"from the R5 (2023-09) to the R6 (2024-03) state"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The document explains how the management cluster and the workload clusters can be\nupgraded without disruption. It is highly recommended to do a step-by-step upgrade\nacross major releases i.e. upgrade from R2 to R3 and then to R4 in the case of\nupgrade from the R2 to the R4. Upgrades across major releases without step-by-step\nprocess is not recommended and could lead to undocumented issues."}),"\n",(0,t.jsx)(n.p,{children:"The various steps are not very complicated, but there are numerous steps to\ntake, and it is advisable that cluster operators get some experience with\nthis kind of cluster management before applying this to customer clusters\nthat carry important workloads."}),"\n",(0,t.jsx)(n.p,{children:"Note that while the detailed steps are tested and targeted to an R2 -> R3 move,\nR3 -> R4 move, R4 -> R5 move or R5 -> R6 move, many of the steps are a generic approach that will apply also for other\nupgrades, so expect a lot of similar steps when moving beyond R6."}),"\n",(0,t.jsxs)(n.p,{children:["Upgrades from cluster management prior to R2 are difficult; many changes before\nR2 assumed that you would redeploy the management cluster. Redeploying the\nmanagement cluster can of course always be done, but it's typically disruptive\nto your workload clusters, unless you move your cluster management state into\na new management cluster with ",(0,t.jsx)(n.code,{children:"clusterctl move"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"management-host-cluster-vs-workload-clusters",children:"Management host (cluster) vs. Workload clusters"}),"\n",(0,t.jsxs)(n.p,{children:["When you initially deployed the SCS k8s-cluster-api-provider, you created a\nVM with a ",(0,t.jsx)(n.a,{href:"https://kind.sigs.k8s.io/",children:"kind"})," cluster inside and a number of\ntemplates, scripts and binaries that are then used to do the cluster management.\nThis is your management host (or more precisely your single-host management\ncluster). Currently, all cluster management including upgrading etc. is done\nby connecting to this host via ssh and performing commands there. (You don't\nneed root privileges to do cluster management there, the normal ubuntu user\nrights are sufficient; there are obviously host management tasks such as\ninstalling package updates that do require root power and the user has the\nsudo rights to do so.)"]}),"\n",(0,t.jsxs)(n.p,{children:["When you create the management host, you have the option to create your\nfirst workload cluster. This cluster is no different from other workload\nclusters that you create by calling commands on the management host, so you\ncan manage it there. (The default name of this cluster is typically\n",(0,t.jsx)(n.code,{children:"testcluster"}),", though that can be changed since a while, #264)."]}),"\n",(0,t.jsxs)(n.p,{children:["On the management host, you have the openstack and kubernetes tools\ninstalled and configured, so you can nicely manage all aspects of your\nCaaS setups as well as the underlying IaaS. The kubectl configuration\nis in ",(0,t.jsx)(n.code,{children:"~/.kube/config"})," while you will find the OpenStack configuration\nin ",(0,t.jsx)(n.code,{children:"~/.config/openstack/clouds.yaml"}),". These files are automatically\nmanaged; you can add entries to the files though, and they should\npersist."]}),"\n",(0,t.jsx)(n.h2,{id:"updating-the-management-host",children:"Updating the management host"}),"\n",(0,t.jsx)(n.p,{children:"There are two different possibilities to upgrade the management host."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"You do a component-wise in-place upgrade of it."}),"\n",(0,t.jsxs)(n.li,{children:["You deploy a new management host and ",(0,t.jsx)(n.code,{children:"clusterctl move"})," the resources\nover to it from the old one. (Note: Config state in ",(0,t.jsx)(n.code,{children:"~/CLUSTER_NAME/"}),")"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"TODO: Advice when to do what, risks, limitations"}),"\n",(0,t.jsx)(n.h3,{id:"in-place-upgrade",children:"In-place upgrade"}),"\n",(0,t.jsx)(n.h4,{id:"operating-system",children:"Operating system"}),"\n",(0,t.jsxs)(n.p,{children:["You should keep the host up-to-date with respect to normal operating system\nupgrades, so perform your normal ",(0,t.jsx)(n.code,{children:"sudo apt-get update && sudo apt-get upgrade"}),".\n",(0,t.jsx)(n.code,{children:"kubectl"}),", ",(0,t.jsx)(n.code,{children:"kustomize"}),", ",(0,t.jsx)(n.code,{children:"yq"}),", ",(0,t.jsx)(n.code,{children:"lxd"})," and a few other tools are installed as\nsnaps, so you may want to upgrade these as well: ",(0,t.jsx)(n.code,{children:"sudo snap refresh"}),".\nFrom R5 ",(0,t.jsx)(n.code,{children:"sudo apt-get install -y jq"})," is also required as this is used by the diskless flavors feature, #424.\nDefault operating system image was changed from Ubuntu 20.04 to Ubuntu 22.04 in R4."]}),"\n",(0,t.jsx)(n.h4,{id:"k8s-cluster-api-provider-git",children:"k8s-cluster-api-provider git"}),"\n",(0,t.jsxs)(n.p,{children:["The automation is deployed on the management host by cloning ",(0,t.jsx)(n.a,{href:"https://github.com/SovereignCloudStack/k8s-cluster-api-provider",children:"the relevant\ngit repository"}),".\ninto the ",(0,t.jsx)(n.code,{children:"k8s-cluster-api-provider"})," directory. Note that the checked out\nbranch will be the one that has been used when creating the management host,\nand you might want to change branches from ",(0,t.jsx)(n.code,{children:"maintained/v3.x"})," to ",(0,t.jsx)(n.code,{children:"maintained/v4.x"}),"\nin case of R2 to R3 upgrade, ",(0,t.jsx)(n.code,{children:"maintained/v5.x"})," for R3 to R4 upgrade, ",(0,t.jsx)(n.code,{children:"maintained/v6.x"}),"\nfor R4 to R5 upgrade or ",(0,t.jsx)(n.code,{children:"maintained/v7.x"})," for R5 to R6 upgrade.\nUse ",(0,t.jsx)(n.code,{children:"git branch -rl"})," to see available branches in the k8s-cluster-api-provider\nrepository."]}),"\n",(0,t.jsxs)(n.p,{children:["You can update the scripts and templates by checking out the relevant branch\n",(0,t.jsx)(n.code,{children:"main"}),", ",(0,t.jsx)(n.code,{children:"maintained/v4.x"}),", ",(0,t.jsx)(n.code,{children:"maintained/v5.x"}),", ",(0,t.jsx)(n.code,{children:"maintained/v6.x"})," or ",(0,t.jsx)(n.code,{children:"maintained/v7.x"}),"\nand using a ",(0,t.jsx)(n.code,{children:"git pull"})," to ensure the latest content is retrieved.\nOnce you do that, the cluster-management scripts will be up-to-date.\n(The ",(0,t.jsx)(n.code,{children:"~/bin"})," directory in your search ",(0,t.jsx)(n.code,{children:"PATH"})," is symlinked to the check-ed out scripts.)"]}),"\n",(0,t.jsx)(n.p,{children:"Note however that the binaries and used templates are NOT automatically updated.\nThis should not normally result in problems -- when new features are introduced\nin the management scripts, they ensure to continue to support older templates."}),"\n",(0,t.jsx)(n.h4,{id:"updating-cluster-api-and-openstack-cluster-api-provider",children:"Updating cluster-API and openstack cluster-API provider"}),"\n",(0,t.jsxs)(n.p,{children:["To get the latest version of cluster-API, you can download a new clusterctl\nbinary from ",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/cluster-api/releases",children:"https://github.com/kubernetes-sigs/cluster-api/releases"}),",\nmake it executable ",(0,t.jsx)(n.code,{children:"chmod +x clusterctl"})," and install it to ",(0,t.jsx)(n.code,{children:"/usr/local/bin/"}),",\npossibly saving the old binary by renaming it. ",(0,t.jsx)(n.code,{children:"clusterctl version"})," should now\ndisplay the current version number (v1.6.2 at the time of this writing)."]}),"\n",(0,t.jsxs)(n.p,{children:["You can now issue the command ",(0,t.jsx)(n.code,{children:"clusterctl upgrade plan"})," and clusterctl will\nlist the components in your (kind) management cluster that can be upgraded.\nHere's an example output:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ubuntu@capi-old-mgmtcluster:~ [0]$ clusterctl upgrade plan\nChecking cert-manager version...\nCert-Manager is already up to date\n\nChecking new release availability...\n\nLatest release available for the v1beta1 API Version of Cluster API (contract):\n\nNAME                       NAMESPACE                           TYPE                     CURRENT VERSION   NEXT VERSION\nbootstrap-kubeadm          capi-kubeadm-bootstrap-system       BootstrapProvider        v1.5.1            v1.6.2\ncontrol-plane-kubeadm      capi-kubeadm-control-plane-system   ControlPlaneProvider     v1.5.1            v1.6.2\ncluster-api                capi-system                         CoreProvider             v1.5.1            v1.6.2\ninfrastructure-openstack   capo-system                         InfrastructureProvider   v0.7.3            v0.9.0\n\nYou can now apply the upgrade by executing the following command:\n\nclusterctl upgrade apply --contract v1beta1\n"})}),"\n",(0,t.jsx)(n.p,{children:"You can then upgrade the components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"export CLUSTER_TOPOLOGY=true"})," - this is needed from R5 to R6 upgrade due to ClusterClass feature #600"]}),"\n",(0,t.jsxs)(n.li,{children:["Upgrade components","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["You can do them one-by-one, e.g.:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"clusterctl upgrade apply --infrastructure capo-system/openstack:v0.9.0 --core capi-system/cluster-api:v1.6.2 -b capi-kubeadm-bootstrap-system/kubeadm:v1.6.2 -c capi-kubeadm-control-plane-system/kubeadm:v1.6.2\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Or simply do ",(0,t.jsx)(n.code,{children:"clusterctl upgrade apply --contract v1beta1"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"new-templates",children:"New templates"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," template used for the workload clusters is located in\n",(0,t.jsx)(n.code,{children:"~/k8s-cluster-api-provider/terraform/files/template/"})," and copied from there into\n",(0,t.jsx)(n.code,{children:"~/cluster-defaults/"}),". Then workload clusters are created, they will also have a\ncopy of it in ",(0,t.jsx)(n.code,{children:"~/${CLUSTER_NAME}/"}),". If you have not changed it manually, you can\ncopy it over the old templates. (Consider backing up the old one though.)"]}),"\n",(0,t.jsxs)(n.p,{children:["The next ",(0,t.jsx)(n.code,{children:"create_cluster.sh <CLUSTER_NAME>"})," run will then use the new template.\nNote that ",(0,t.jsx)(n.code,{children:"create_cluster.sh"})," is idempotent -- it will not perform any changes\non the cluster unless you have changed its configuration by tweaking\n",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," (which you almost never do!) or ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"}),"\n(which you do often)."]}),"\n",(0,t.jsx)(n.p,{children:"The other template file that changed -- however, some opentofu logic is used to\nprefill it with values. So you can't copy it from git."}),"\n",(0,t.jsx)(n.h5,{id:"r2-to-r3",children:"R2 to R3"}),"\n",(0,t.jsxs)(n.p,{children:["For going from R2 to R3, there is just one real change that you want\nto apply: Add the variables ",(0,t.jsx)(n.code,{children:"CONTROL_PLANE_MACHINE_GEN: genc01"})," and\n",(0,t.jsx)(n.code,{children:"WORKER_MACHINE_GEN: genw01"})," to it. If you have copied over the new\n",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," as described above, then you're done. Otherwise\nyou can use the script ",(0,t.jsx)(n.code,{children:"update-R2-R3.sh <CLUSTER_NAME>"}),"\nto tweak both ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"})," and ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," for the\nrelevant cluster. (You can use ",(0,t.jsx)(n.code,{children:"cluster-defaults"})," to change the templates\nin ",(0,t.jsx)(n.code,{children:"~/cluster-defaults/"})," which get used when creating new clusters.)"]}),"\n",(0,t.jsx)(n.h5,{id:"r3-to-r4",children:"R3 to R4"}),"\n",(0,t.jsxs)(n.p,{children:["In the R3 to R4, CALICO_VERSION was moved from ",(0,t.jsx)(n.code,{children:".capi-settings"})," to ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"}),". So\nbefore upgrading workload clusters, you must add it also to the ",(0,t.jsx)(n.code,{children:"~/${CLUSTER_NAME}/clusterctl.yaml"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'echo "CALICO_VERSION: v3.25.0" >> ~/cluster-defaults/clusterctl.yaml\necho "CALICO_VERSION: v3.25.0" >> ~/testcluster/clusterctl.yaml\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In the R3 to R4 upgrade process, ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," changed etcd defrag process in the\nkubeadm control-planes and also security group names by adding ",(0,t.jsx)(n.code,{children:"${PREFIX}-"})," to them, so it\nhas to be changed also in openstack project e.g. (",(0,t.jsx)(n.em,{children:"PREFIX=capi"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"openstack security group set --name capi-allow-ssh allow-ssh\nopenstack security group set --name capi-allow-icmp allow-icmp\n"})}),"\n",(0,t.jsxs)(n.p,{children:["We changed immutable fields in the Cluster API templates, so before running\n",(0,t.jsx)(n.code,{children:"create_cluster.sh"})," to upgrade existing workload cluster the ",(0,t.jsx)(n.code,{children:"CONTROL_PLANE_MACHINE_GEN"}),"\nand ",(0,t.jsx)(n.code,{children:"WORKER_MACHINE_GEN"})," needs to be incremented in cluster specific ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["In the R3 to R4 process, also ",(0,t.jsx)(n.code,{children:"cloud.conf"})," added ",(0,t.jsx)(n.code,{children:"enable-ingress-hostname=true"})," to the\nLoadBalancer section. It is due to ",(0,t.jsx)(n.code,{children:"NGINX_INGRESS_PROXY"})," defaulting to true now. So if\nyou want to use, or you are already using this proxy functionality we recommend you to\nadd this line to your ",(0,t.jsx)(n.code,{children:"cloud.conf"}),", e.g.:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'echo "enable-ingress-hostname=true" >> ~/cluster-defaults/cloud.conf\necho "enable-ingress-hostname=true" >> ~/testcluster/cloud.conf\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Then, before upgrading workload cluster by ",(0,t.jsx)(n.code,{children:"create_cluster.sh"}),",\nyou should delete cloud-config secret in the kube-system namespace, so it can be recreated. E.g.:\n",(0,t.jsx)(n.code,{children:"kubectl delete secret cloud-config -n kube-system --kubeconfig=testcluster/testcluster.yaml"})]}),"\n",(0,t.jsx)(n.p,{children:"Also, the default nginx-ingress version has changed, so we recommend before upgrading cluster\nto delete ingress-nginx jobs, so new job with new image can be created in the update process."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl delete job ingress-nginx-admission-create -n ingress-nginx --kubeconfig=testcluster/testcluster.yaml\nkubectl delete job ingress-nginx-admission-patch -n ingress-nginx --kubeconfig=testcluster/testcluster.yaml\n"})}),"\n",(0,t.jsx)(n.h5,{id:"r4-to-r5",children:"R4 to R5"}),"\n",(0,t.jsxs)(n.p,{children:["In R4 to R5, the ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," and ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"})," changed (see release notes).\nYou can use script ",(0,t.jsx)(n.code,{children:"update-R4-to-R5.sh"})," to update the cluster's ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," and ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"})," from\nR4 to R5. This script could update an existing Kubernetes cluster configuration files\nas well as ",(0,t.jsx)(n.code,{children:"cluster-defaults"})," files that could be used for spawning new R5 clusters."]}),"\n",(0,t.jsx)(n.p,{children:"If you want to update an existing cluster configuration files from R4 to R5, just use script as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"update-R4-to-R5.sh <CLUSTER_NAME>\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After you executed the above you will find that e.g. Calico version has been bumped from\nv3.25.0 to v3.26.1. Note that some software versions are not configurable and are not\ndirectly mentioned in the cluster configuration files, but they are hardcoded\nin R5 scripts (e.g. ingress nginx controller, metrics server), see ",(0,t.jsx)(n.a,{href:"#new-defaults",children:"new-defaults"})," section.\nNote that the Kubernetes version was not updated as well the default CNI is not the Cilium yet.\nThis two R5 features are out of scope this script when it is applied on the existing cluster\nconfiguration files as this features require advanced action such as CNI migration\nand step-by-step Kubernetes upgrade of +2 minor releases."]}),"\n",(0,t.jsxs)(n.p,{children:["If you want to update ",(0,t.jsx)(n.code,{children:"cluster-defaults"})," configuration files from R4 to R5, just use script as follows:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"update-R4-to-R5.sh cluster-defaults\n"})}),"\n",(0,t.jsx)(n.p,{children:"The above action updates a cluster-defaults configuration file, which is almost similar\nto updating an existing cluster configuration file described above. The distinction lies\nin the fact that both the Kubernetes version and the default CNI are also updated, specifically\nto Kubernetes version v1.27.5 and Cilium as a default CNI."}),"\n",(0,t.jsx)(n.h5,{id:"r5-to-r6",children:"R5 to R6"}),"\n",(0,t.jsxs)(n.p,{children:["In R5 to R6, the ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," and ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"})," changed (see release notes).\nYou can use script ",(0,t.jsx)(n.code,{children:"update-R5-to-R6.sh"})," to update the cluster's ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," and ",(0,t.jsx)(n.code,{children:"clusterctl.yaml"})," from\nR5 to R6. This script could update an existing Kubernetes cluster configuration files\nas well as ",(0,t.jsx)(n.code,{children:"cluster-defaults"})," files that could be used for spawning new R6 clusters."]}),"\n",(0,t.jsx)(n.p,{children:"If you want to update an existing cluster configuration files from R5 to R6, just use script as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"update-R5-to-R6.sh <CLUSTER_NAME>\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After you executed the above you will find that e.g. Calico version has been bumped from\nv3.26.1 to v3.27.2 or Kubernetes version bumped from v1.27.5 to v1.28.7.\nNote that some software versions are not configurable and are not directly mentioned\nin the cluster configuration files, but they are hardcoded\nin R6 scripts (e.g. ingress nginx controller, metrics server, cilium), see ",(0,t.jsx)(n.a,{href:"#new-defaults",children:"new-defaults"})," section."]}),"\n",(0,t.jsxs)(n.p,{children:["If you want to update ",(0,t.jsx)(n.code,{children:"cluster-defaults"})," configuration files from R5 to R6, just use script as follows:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"update-R5-to-R6.sh cluster-defaults\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If you are curious: In R2, doing rolling upgrades of k8s versions required\nedits in ",(0,t.jsx)(n.code,{children:"cluster-template.yaml"})," -- this is no longer the case in R3, R4, R5 and R6.\nJust increase the generation counter for node and control plane nodes if you\nupgrade k8s versions -- or otherwise change the worker or control plane\nnode specs, such as e.g. using a different flavor."]}),"\n",(0,t.jsx)(n.h4,{id:"new-defaults",children:"New defaults"}),"\n",(0,t.jsxs)(n.p,{children:["You deploy a CNI (calico or cilium), the OpenStack Cloud Controller\nManager (OCCM), the cinder CSI driver to clusters; optionally also a\nmetrics server (default is true), a nginx ingress controller (also\ndefaulting to true), the flux2 controller, the cert-manager.\nSome of these tools come with binaries that you can use for management\npurposes and which get installed on the management host in ",(0,t.jsx)(n.code,{children:"/usr/local/bin/"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["The scripts that deploy these components into your workload clusters\ndownload the manifests into ",(0,t.jsx)(n.code,{children:"~/kubernetes-manifests.d/"})," with a version\nspecific name. If you request a new version, a new download will happen;\nalready existing versions will not be re-downloaded."]}),"\n",(0,t.jsxs)(n.p,{children:["Most binaries in ",(0,t.jsx)(n.code,{children:"/usr/local/bin/"})," are not stored under a version-specific\nname. You need to rename them to case a re-download of a newer version.\n(The reason for not having version specific names is that this would\nbreak scripts from users that assume the unversioned names; the good\nnews is that most of these binaries have no trouble managing somewhat\nolder deployments, so you can typically work with the latest binary\ntool even if you have a variety of versions deployed into various\nclusters.)"]}),"\n",(0,t.jsx)(n.p,{children:"The defaults have changed as follows:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{}),(0,t.jsx)(n.th,{children:"R2"}),(0,t.jsx)(n.th,{children:"R3"}),(0,t.jsx)(n.th,{children:"R4"}),(0,t.jsx)(n.th,{children:"R5"}),(0,t.jsx)(n.th,{children:"R6"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"kind"}),(0,t.jsx)(n.td,{children:"v0.14.0"}),(0,t.jsx)(n.td,{children:"v0.14.0"}),(0,t.jsx)(n.td,{children:"v0.17.0"}),(0,t.jsx)(n.td,{children:"v0.20.0"}),(0,t.jsx)(n.td,{children:"v0.20.0"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"capi"}),(0,t.jsx)(n.td,{children:"v1.0.5"}),(0,t.jsx)(n.td,{children:"v1.2.2"}),(0,t.jsx)(n.td,{children:"v1.3.5"}),(0,t.jsx)(n.td,{children:"v1.5.1"}),(0,t.jsx)(n.td,{children:"v1.6.2"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"capo"}),(0,t.jsx)(n.td,{children:"v0.5.3"}),(0,t.jsx)(n.td,{children:"v0.6.3"}),(0,t.jsx)(n.td,{children:"v0.7.1"}),(0,t.jsx)(n.td,{children:"v0.7.3"}),(0,t.jsx)(n.td,{children:"v0.9.0"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"helm"}),(0,t.jsx)(n.td,{children:"v3.8.1"}),(0,t.jsx)(n.td,{children:"v3.9.4"}),(0,t.jsx)(n.td,{children:"v3.11.1"}),(0,t.jsx)(n.td,{children:"v3.12.3"}),(0,t.jsx)(n.td,{children:"v3.14.1"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"sonobuoy"}),(0,t.jsx)(n.td,{children:"v0.56.2"}),(0,t.jsx)(n.td,{children:"v0.56.10"}),(0,t.jsx)(n.td,{children:"v0.56.16"}),(0,t.jsx)(n.td,{children:"v0.56.17"}),(0,t.jsx)(n.td,{children:"v0.57.1"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"k9s"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"v0.27.4"}),(0,t.jsx)(n.td,{children:"v0.31.9"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"calico"}),(0,t.jsx)(n.td,{children:"v3.22.1"}),(0,t.jsx)(n.td,{children:"v3.24.1"}),(0,t.jsx)(n.td,{children:"v3.25.0"}),(0,t.jsx)(n.td,{children:"v3.26.1"}),(0,t.jsx)(n.td,{children:"v3.27.2"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"calico CLI"}),(0,t.jsx)(n.td,{children:"v3.22.1"}),(0,t.jsx)(n.td,{children:"v3.24.1"}),(0,t.jsx)(n.td,{children:"v3.25.0"}),(0,t.jsx)(n.td,{children:"v3.26.1"}),(0,t.jsx)(n.td,{children:"v3.27.2"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"cilium"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"v1.13.0"}),(0,t.jsx)(n.td,{children:"v1.14.1"}),(0,t.jsx)(n.td,{children:"v1.15.1"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"cilium CLI"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"v0.13.1"}),(0,t.jsx)(n.td,{children:"v0.15.7"}),(0,t.jsx)(n.td,{children:"v0.15.23"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"hubble CLI"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"v0.11.2"}),(0,t.jsx)(n.td,{children:"v0.12.0"}),(0,t.jsx)(n.td,{children:"v0.13.0"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"nginx-ingress"}),(0,t.jsx)(n.td,{children:"v1.1.2"}),(0,t.jsx)(n.td,{children:"v1.3.0"}),(0,t.jsx)(n.td,{children:"v1.6.4"}),(0,t.jsx)(n.td,{children:"v1.8.1"}),(0,t.jsx)(n.td,{children:"v1.9.6"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"flux2"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"unversioned"}),(0,t.jsx)(n.td,{children:"v0.40.2"}),(0,t.jsx)(n.td,{children:"v2.1.0"}),(0,t.jsx)(n.td,{children:"v2.2.3"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"cert-manager"}),(0,t.jsx)(n.td,{children:"v1.7.1"}),(0,t.jsx)(n.td,{children:"v1.9.1"}),(0,t.jsx)(n.td,{children:"v1.11.0"}),(0,t.jsx)(n.td,{children:"v1.12.4"}),(0,t.jsx)(n.td,{children:"v1.14.2"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"metrics-server"}),(0,t.jsx)(n.td,{children:"v0.6.1"}),(0,t.jsx)(n.td,{children:"v0.6.1"}),(0,t.jsx)(n.td,{children:"v0.6.1"}),(0,t.jsx)(n.td,{children:"v0.6.4"}),(0,t.jsx)(n.td,{children:"v0.7.0"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"kubectx"}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{children:"v0.9.5"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"kube-ps1"}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{}),(0,t.jsx)(n.td,{children:"v0.8.0"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"the-clusterctl-move-approach",children:"The clusterctl move approach"}),"\n",(0,t.jsx)(n.p,{children:"To be written"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create new management host in same project -- avoid name conflicts\nwith different prefix, to be tweaked later. Avoid testcluster creation"}),"\n",(0,t.jsx)(n.li,{children:"Ensure it's up and running ..."}),"\n",(0,t.jsx)(n.li,{children:"Tweak prefix"}),"\n",(0,t.jsx)(n.li,{children:"Copy over configs (and a bit of state though that's uncritical) by using\nthe directories"}),"\n",(0,t.jsx)(n.li,{children:"Copy over the openstack credentials clouds.yaml and the kubectl config"}),"\n",(0,t.jsx)(n.li,{children:"clusterctl move"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"updating-workload-clusters",children:"Updating workload clusters"}),"\n",(0,t.jsx)(n.h3,{id:"k8s-version-upgrade",children:"k8s version upgrade"}),"\n",(0,t.jsx)(n.h4,{id:"on-r2-clusters",children:"On R2 clusters"}),"\n",(0,t.jsxs)(n.p,{children:["The old way: Editing cluster-template.yaml. Or better use the\n",(0,t.jsx)(n.code,{children:"update-R2-to-R3.sh"})," script to convert first."]}),"\n",(0,t.jsx)(n.h4,{id:"on-r3-and-r4-clusters",children:"On R3 and R4 clusters"}),"\n",(0,t.jsxs)(n.p,{children:["Edit ",(0,t.jsx)(n.code,{children:"~/<CLUSTER_NAME>/clusterctl.yaml"})," and put the wanted version into the\nfields ",(0,t.jsx)(n.code,{children:"KUBERNETES_VERSION"})," and ",(0,t.jsx)(n.code,{children:"OPENSTACK_IMAGE_NAME"}),". The node image will\nbe downloaded from ",(0,t.jsx)(n.a,{href:"https://minio.services.osism.tech/openstack-k8s-capi-images",children:"https://minio.services.osism.tech/openstack-k8s-capi-images"}),"\nand registered if needed. (If you have updated the k8s-cluster-api-provider repo,\nyou can use a version v1.NN.x, where you fill in NN with the wanted k8s version,\nbut leave a literal ",(0,t.jsx)(n.code,{children:".x"})," which will get translated to the newest tested version.)"]}),"\n",(0,t.jsxs)(n.p,{children:["In the same file, increase the generation counters for ",(0,t.jsx)(n.code,{children:"CONTROL_PLANE_MACHINE_GEN"}),"\nand ",(0,t.jsx)(n.code,{children:"WORKER_MACHINE_GEN"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Now do the normal ",(0,t.jsx)(n.code,{children:"create_cluster.sh <CLUSTER_NAME>"})," and watch cluster-api\nreplace your worker nodes and doing a rolling upgrade of your control plane.\nIf you used a 3-node (or 5 or higher) control plane node setup, you will have\nuninterrupted access not just to your workloads but also the workload's cluster\ncontrol plane. Use ",(0,t.jsx)(n.code,{children:"clusterctl describe cluster <CLUSTER_NAME>"})," or simply\n",(0,t.jsx)(n.code,{children:"kubectl --context <CLUSTER_NAME>-admin@<CLUSTER_NAME> get nodes -o wide"}),"\nto watch the progress of this."]}),"\n",(0,t.jsx)(n.h4,{id:"on-r5-clusters",children:"On R5 clusters"}),"\n",(0,t.jsx)(n.p,{children:"If you decide to migrate your existing Kubernetes cluster from R4 to R5 be aware of the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"R5 features such as per cluster namespaces and Cilium as a default CNI are supported\nonly on new clusters and will not be migrated on the existing clusters"}),"\n",(0,t.jsxs)(n.li,{children:["R4 default Kubernetes version v1.25.6 can not be directly migrated to the R5 default\nKubernetes version v1.27.5, because +2 minor Kubernetes version upgrade is ",(0,t.jsx)(n.a,{href:"https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/operate-cluster.html?highlight=upgrade%20cluster#upgrade-a-cluster",children:"not allowed"}),".\nSee further migration steps below if you want to upgrade also Kubernetes version to R5"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Follow the below steps if you want to migrate an existing cluster from R4 to R5:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Access your management node"}),"\n",(0,t.jsxs)(n.li,{children:["Checkout R5 branch","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd k8s-cluster-api-provider\ngit pull\ngit checkout maintained/v6.x\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Backup an existing cluster configuration files (recommended)","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ..\ncp -R <CLUSTER_NAME> <CLUSTER_NAME>-backup\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Update an existing cluster configuration files from R4 to R5","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"update-R4-to-R5.sh <CLUSTER_NAME>\n"})}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Validate updated cluster configuration files. You will find that e.g. Calico version\nhas been bumped from v3.25.0 to v3.26.1. Note that some software versions are not configurable\nand are not directly mentioned in the cluster configuration files, but they are hardcoded\nin R5 scripts (e.g. ingress nginx controller, metrics server). Hence, read carefully the\nR5 release notes too. Also see that Kubernetes version was not updated, and it is still v1.25.6."}),"\n",(0,t.jsxs)(n.li,{children:["Update an existing cluster (except Kubernetes version)","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"create_cluster.sh <CLUSTER_NAME>\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Update cluster-API and openstack cluster-API provider, see ",(0,t.jsx)(n.a,{href:"#updating-cluster-api-and-openstack-cluster-api-provider",children:"related"})," section for details","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"clusterctl upgrade plan\nclusterctl upgrade apply --contract v1beta1\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Bump Kubernetes version +1 minor release (to v1.26.8) and increase the generation counter for node and control plane nodes","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sed -i 's/^KUBERNETES_VERSION: v1.25.6/KUBERNETES_VERSION: v1.26.8/' <CLUSTER_NAME>/clusterctl.yaml\nsed -i 's/^OPENSTACK_IMAGE_NAME: ubuntu-capi-image-v1.25.6/OPENSTACK_IMAGE_NAME: ubuntu-capi-image-v1.26.8/' <CLUSTER_NAME>/clusterctl.yaml\nsed -r 's/(^CONTROL_PLANE_MACHINE_GEN: genc)([0-9][0-9])/printf \"\\1%02d\" $((\\2+1))/ge' -i <CLUSTER_NAME>/clusterctl.yaml\nsed -r 's/(^WORKER_MACHINE_GEN: genw)([0-9][0-9])/printf \"\\1%02d\" $((\\2+1))/ge' -i <CLUSTER_NAME>/clusterctl.yaml\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Update an existing cluster Kubernetes version to v1.26.8","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"create_cluster.sh <CLUSTER_NAME>\n"})}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Bump Kubernetes version to R5 v1.27.5 and increase the generation counter for node and control plane nodes"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sed -i 's/^KUBERNETES_VERSION: v1.26.8/KUBERNETES_VERSION: v1.27.5/' <CLUSTER_NAME>/clusterctl.yaml\nsed -i 's/^OPENSTACK_IMAGE_NAME: ubuntu-capi-image-v1.26.8/OPENSTACK_IMAGE_NAME: ubuntu-capi-image-v1.27.5/' <CLUSTER_NAME>/clusterctl.yaml\nsed -r 's/(^CONTROL_PLANE_MACHINE_GEN: genc)([0-9][0-9])/printf \"\\1%02d\" $((\\2+1))/ge' -i <CLUSTER_NAME>/clusterctl.yaml\nsed -r 's/(^WORKER_MACHINE_GEN: genw)([0-9][0-9])/printf \"\\1%02d\" $((\\2+1))/ge' -i <CLUSTER_NAME>/clusterctl.yaml\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"11",children:["\n",(0,t.jsxs)(n.li,{children:["Update an existing cluster to the R5 Kubernetes version v1.27.5","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"create_cluster.sh <CLUSTER_NAME>\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"on-r6-clusters",children:"On R6 clusters"}),"\n",(0,t.jsx)(n.p,{children:"If you decide to migrate your existing Kubernetes cluster from R5 to R6 be aware of the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Kubernetes version will be upgraded from v1.27.5 to v1.28.7"}),"\n",(0,t.jsx)(n.li,{children:"You have to migrate from Cluster based templates to ClusterClass based templates"}),"\n",(0,t.jsxs)(n.li,{children:["Upgrade of cilium needs to be done manually (for clusters with ",(0,t.jsx)(n.code,{children:"USE_CILIUM: true"}),")"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Follow the below steps if you want to migrate an existing cluster from R5 to R6:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Access your management node"}),"\n",(0,t.jsxs)(n.li,{children:["Checkout R6 branch","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd k8s-cluster-api-provider\ngit pull\ngit checkout maintained/v7.x\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Backup an existing cluster configuration files (recommended)","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ..\ncp -R <CLUSTER_NAME> <CLUSTER_NAME>-backup\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Update an existing cluster configuration files from R5 to R6","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"update-R5-to-R6.sh <CLUSTER_NAME>\n"})}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Validate updated cluster configuration files. You will find that e.g. Calico version\nhas been bumped from v3.26.1 to v3.27.2 or Kubernetes version bumped from v1.27.5 to v1.28.7.\nNote that some software versions are not configurable and are not directly mentioned in the\ncluster configuration files, but they are hardcoded in R6 scripts (e.g. ingress nginx controller,\nmetrics server, cilium). Hence, read carefully the R6 release notes too."}),"\n",(0,t.jsxs)(n.li,{children:["Update cluster-API and openstack cluster-API provider, see ",(0,t.jsx)(n.a,{href:"#updating-cluster-api-and-openstack-cluster-api-provider",children:"related"})," section for details","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"clusterctl upgrade plan\nexport CLUSTER_TOPOLOGY=true\nclusterctl upgrade apply --contract v1beta1\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Migrate to ClusterClass","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"migrate-to-cluster-class.sh <CLUSTER_NAME>\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Increase the generation counter for worker and control plane nodes","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sed -r 's/(^CONTROL_PLANE_MACHINE_GEN: genc)([0-9][0-9])/printf \"\\1%02d\" $((\\2+1))/ge' -i <CLUSTER_NAME>/clusterctl.yaml\nsed -r 's/(^WORKER_MACHINE_GEN: genw)([0-9][0-9])/printf \"\\1%02d\" $((\\2+1))/ge' -i <CLUSTER_NAME>/clusterctl.yaml\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Update an existing cluster to the R6","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"create_cluster.sh <CLUSTER_NAME>\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Note: You will probably experience a double rollout of nodes because\nthe k8s version and templates are changed concurrently here.\nSee ",(0,t.jsx)(n.a,{href:"https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/operate-cluster#effects-of-concurrent-changes",children:"https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/operate-cluster#effects-of-concurrent-changes"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Upgrade cilium (for clusters with ",(0,t.jsx)(n.code,{children:"USE_CILIUM: true"}),")","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"KUBECONFIG=<CLUSTER_NAME>/<CLUSTER_NAME>.yaml bash -c 'helm get values cilium -n kube-system -o yaml | cilium upgrade --version v1.15.1 -f -'\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"new-versions-for-mandatory-components",children:"New versions for mandatory components"}),"\n",(0,t.jsx)(n.p,{children:"OCCM, CNI (calico/cilium), CSI"}),"\n",(0,t.jsx)(n.h3,{id:"new-versions-for-optional-components",children:"New versions for optional components"}),"\n",(0,t.jsx)(n.p,{children:"nginx, metrics server, cert-manager, flux"}),"\n",(0,t.jsx)(n.h3,{id:"etcd-leader-changes",children:"etcd leader changes"}),"\n",(0,t.jsx)(n.p,{children:"While testing clusters with >= 3 control nodes, we have observed occasional transient\nerror messages that reported an etcd leader change preventing a command from succeeding.\nThis could result in a dozen of random failed tests in a sonobuoy conformance run.\n(Retrying the commands would let them succeed.)"}),"\n",(0,t.jsx)(n.p,{children:"Too frequent etcd leader changes are detrimental to your control plane performance and\ncan lead to transient failures. They are a sign that the infrastructure supporting your\ncluster is introducing too high latencies."}),"\n",(0,t.jsx)(n.p,{children:"We recommend to deploy the control nodes (which run etcd) on instances with local SSD\nstorage (which we reflect in the default flavor name) and recommend using flavors with\ndedicated cores and that your network does not introduce latencies by significant packet drop."}),"\n",(0,t.jsxs)(n.p,{children:["We now always use slower heartbeat (250ms) and increase CPU and IO priority which used to be\ncontrolled by ",(0,t.jsx)(n.code,{children:"ETCD_PRIO_BOOST"}),". This is safe."]}),"\n",(0,t.jsxs)(n.p,{children:["If you build multi-controller clusters and can not use a flavor with low latency local storage\n(ideally SSD), you can also work around this with ",(0,t.jsx)(n.code,{children:"ETCD_UNSAFE_FS"}),". ",(0,t.jsx)(n.code,{children:"ETCD_UNSAFE_FS"})," is using\n",(0,t.jsx)(n.code,{children:"barrier=0"})," mount option, which violates filesystem ordering guarantees.\nThis works around storage latencies, but introduces the risk of inconsistent\nfilesystem state and inconsistent etcd data in case of an unclean shutdown.\nYou may be able to live with this risk in a multi-controller etcd setup.\nIf you don't have flavors that fulfill the requirements (low-latency\nstorage attached), your choice is between a single-controller cluster\n(without ",(0,t.jsx)(n.code,{children:"ETCD_UNSAFE_FS"}),") and a multi-controller cluster with\n",(0,t.jsx)(n.code,{children:"ETCD_UNSAFE_FS"}),". Neither option is perfect, but we deem the\nmulti-controller cluster preferable in such a scenario."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var t=s(96540);const r={},i=t.createContext(r);function l(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);